\documentclass[12pt]{article}
\usepackage[margin=0.75in]{geometry}
\usepackage{graphicx}
\usepackage{amsmath,amsthm,amssymb,amsfonts, bm}
\usepackage{enumitem, nth}
\usepackage{mathtools}

\usepackage{listings}
\usepackage{color}
 
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
 
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4
}
 
\lstset{style=mystyle}
\newcommand*{\myfont}{\fontfamily{<qcr>}\selectfont}
\newcommand\at[2]{\left.#1\right|_{#2}}

\begin{document}
\title{CS224n HW2}
\author{Xinyu Tan}
\maketitle

%------------------------------------------------------
% problem 2: Neural Transition-based Dependency Parsing
%------------------------------------------------------
\section {Neural Transition-based Dependency Parsing}
\subsection*{(a)}

\begin{center}
\begin{tabular}{c|c|c|c}
stack & buffer & new dependency & transition\\
\hline
[root] & [I, parsed, this, sentence, correctly] &  & Initial Configuration \\
{[root, I]} & [parsed, this, sentence, correctly] &  & {\myfont SHIFT } \\
{[root, I, parsed]} & [this, sentence, correctly] &  & {\myfont SHIFT} \\ 
{[root, parsed]} & [this, sentence, correctly] & parsed $\rightarrow$ I & {\myfont LEFT-ARC} \\ 
{[root, parsed, this]} & [sentence, correctly] &  & {\myfont SHIFT} \\
{[root, parsed, this, sentence]} & [correctly] &  & {\myfont SHIFT} \\
{[root, parsed, sentence]} & [correctly] & sentence $\rightarrow$ this & {\myfont LEFT-ARC} \\
{[root, parsed]} & [correctly] & parsed $\rightarrow$ sentence  & {\myfont RIGHT-ARC} \\
{[root, parsed, correctly]} & [] &  & {\myfont SHIFT} \\
{[root, parsed]} & [] & parsed $\rightarrow$ correctly  & {\myfont RIGHT-ARC} \\
{[root]} & [] & root $\rightarrow$ parsed  & {\myfont RIGHT-ARC} \\
\end{tabular}
\end{center}

\subsection*{(b)}

The sentence will be parsed in $2n$ times. Each word will be pushed into stack once, and each word only depends on one other word. Therefore, the process is in $O(n)$ time complexity.   

\subsection*{(f)}

We need to satisfy: 
$\mathbb{E}_{p_{\mathrm{drop}}}[\bm h_{\mathrm{drop}}]_i = \gamma (1-p_{\mathrm{drop}}) \bm h_i = \bm h_i$, then we have:  
                        $$\gamma = \frac{1}{1 - p_{\mathrm{drop}}}$$


\subsection*{(g)}
\subsubsection*{(i)}

\subsubsection*{(ii)}

%---------------------------------------------------------
% problem 3: Recurrant neural networks: Language Modeling 
%---------------------------------------------------------
\section{Recurrant neural networks: Language Modeling}

\subsection*{(a)}
Perplexity:
$$
PP^{(t)} \left (y^{(t)}, \hat y^{(t)} \right) = \frac{1}{y_k^{(t)} \hat y_k^{(t)}} = \frac{1}{\hat y_k^{(t)}}
$$
Cross-entropy loss:
$$
J^{(t)}(\theta) = -y_k^{(t)} \log \hat y_k^{(t)} = -\log \hat y_k^{(t)}
$$
Then, it is easy to derive that
$$
PP^{(t)} \left (y^{(t)}, \hat y^{(t)} \right) = e^{J^{(t)}(\theta)}
$$
Therefore, minimizing perplexity equals to minimizing the cross-entropy.

For a vocabulary of $|V| = 10000$ words, if the model is completely random, then the perplexity will be 10000, and then the cross entropy will be $\log 10000 = 9.21$.

\section*{(b)}

The derivatives:
$$
\frac{\partial J^{(t)}}{\partial \bm b_2} = \frac{\partial J^{(t)}}{\partial \bm \theta} \frac{\partial \bm \theta}{\partial \bm b_2} = \bm {\hat y}^{(t)} - \bm y^{(t)}
$$
Since $\bm \theta = \bm h^{(t)} U + \bm b_2$, every $\theta_i$ depends on every $h_j$, i.e.,
$$
\frac{\partial J}{\partial h_j^{(t)}} = \sum_{i=1}^{|V|} \frac{\partial J}{\partial \theta_i} \frac{\partial \theta_i}{\partial h_j^{(t)}} = \sum_{i=1}^{|V|} (\hat y_i - y_i) U_{ji}
$$
Hence, we have:
$$
\bm {\delta ^{(t)}} = \frac{\partial J^{(t)}}{\partial \bm h^{(t)}} =  \frac{\partial J^{(t)}}{\partial \bm \theta} \frac{\partial \bm \theta}{\partial \bm h^{(t)}} = (\bm {\hat y} - \bm y) \bm U^{T}
$$

Often times, when it's a lot of matrix multiplications, it's very hard to know when to use matrix multiplication, when to use $\odot$ (element multiplication). I find it helpful to write down the element-wise formula, then it's clearer which affects which. 

Next, let's calculate $\frac{\partial J^{(t)}}{\partial \bm H}$. We have $\bm h^{(t)} = \sigma (\bm h^{(t-1)} \bm H + \bm e^{(t)} \bm I + \bm b_1)$, element wise:
$$
h_j^{(t)} = \sigma \left(  \sum_{k=1}^{D_n} h_k^{(t-1)} H_{kj} + \sum_{k=1}^{d} e_k^{(t)} I_{kj} + b_{1j}\right)
$$
Conceptually,
$$
\frac{\partial J^{(t)}}{\partial H_{kj}} = \frac{\partial J^{(t)}}{\partial h_j^{(t)}} \frac{\partial h_j^{(t)}}{\partial H_{kj}}  
$$
Therefore, 
$$
\frac{\partial J^{(t)}}{\partial \bm H} = {\bm h^{(t-1)}}^T \cdot \left (\bm {\delta ^{(t)}} \odot \bm h^{(t)} \odot \left (1- \bm h^{(t)} \right) \right )
$$
Similarly, we have 
$$
\frac{\partial J^{(t)}}{\partial \bm I} = {\bm e^{(t)}}^T \cdot \left (\bm {\delta ^{(t)}} \odot \bm h^{(t)} \odot \left (1- \bm h^{(t)} \right) \right )
$$
$$
\frac{\partial J^{(t)}}{\partial \bm L_{x^{(t)}}} ={ \frac{\partial J^{(t)}}{\partial \bm e^{(t)}} }^T={ \bm {\delta ^{(t)}} \odot \bm h^{(t)} \odot \left (1- \bm h^{(t)} \right ) \cdot \bm I^T}^T = \bm I \cdot \left (\bm {\delta ^{(t)}} \odot \bm h^{(t)} \odot \left (1- \bm h^{(t)} \right ) \right)^T
$$

Additionally, 
$$
\bm {\delta ^{(t-1)}} = \frac{\partial J^{(t)}}{\partial \bm h^{(t-1)}} =  \sum_{k=1}^{D_h} \frac{\partial J^{(t)}}{\partial h_k^{(t)}} \frac{\partial h_k^{(t)}}{\partial \bm h^{(t-1)}} = \left(\bm \delta^{(t)}  \odot \bm h^{(t)} \odot \left (1- \bm h^{(t)} \right) \right) \bm H^T
$$

\section*{(c)}
%-----1-----
$$\at{\frac{\partial J^{(t)}}{\partial \bm H}}{(t-1)} = \frac{\partial J^{(t)}}{\partial \bm h^{(t-1)}} \frac{\partial \bm h^{(t-1)}}{\partial \bm H} = {\bm h^{(t-2)}}^T \cdot \left (\bm {\delta ^{(t-1)}} \odot \bm h^{(t-1)} \odot \left (1- \bm h^{(t-1)} \right) \right )$$
%-----2-----
$$
\at{\frac{\partial J^{(t)}}{\partial \bm I}}{(t-1)} = \frac{\partial J^{(t)}}{\partial \bm h^{(t-1)}} \frac{\partial \bm h^{(t-1)}}{\partial \bm I} = {\bm e^{(t-1)}}^T \cdot \left (\bm {\delta ^{(t-1)}} \odot \bm h^{(t-1)} \odot \left (1- \bm h^{(t-1)} \right) \right) 
$$
%-----3-----
$$
\at{\frac{\partial J^{(t)}}{\partial \bm L_{x^{t-1}}}}{(t-1)} = \at{\frac{\partial J^{(t)}}{\partial \bm e^{(t-1)}}}{(t-1)} = \frac{\partial J^{(t)}}{\partial \bm h^{(t-1)}} \frac{\partial \bm h^{(t-1)}}{\partial \bm e^{(t-1)}} = \left (\bm {\delta ^{(t-1)}} \odot \bm h^{(t-1)} \odot \left (1- \bm h^{(t-1)} \right) \right) \cdot \bm I^T
$$

\subsection*{(d)}

Given $\bm h^{(t-1)}$, forward pass requires to compute: $\bm h^{(t)} = \sigma \left (\bm h^{(t-1)} \bm H + \bm e^{(t)} \bm I  + b_1\right )$ and $\hat y^{t} = \text{softmax} \left(\bm h^{(t)} \bm U + b_2 \right)$. We know that for a $m\times n$ and $n \times l$ matrix multiplication, the complexity is $O(mnl)$. Therefore, the forward pass complexity is:
$$
O(D_h^2 + dD_h + D_h + D_h|V| + |V|) \approx O(D_h^2 + dD_h + D_h|V|)
$$
Similarly, the backward pass complexity (from $\bm \delta^{(t)}$) is approximately:
$$
 O(D_h^2 + dD_h + D_h|V|)
$$

Due to that $|V| >> D_h \text{ or } d$, therefore, the major time consuming step is softmax ($O(D_h|V|)$)
\end{document}
